{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = './style/armchair.jpg'\n",
    "\n",
    "path = '/home/jovyan/Keras-Real-Time-Style-Transfer/VOCdevkit/VOC2012/JPEGImages/*.jpg'\n",
    "img_rows = 672//2 # 672, 376\n",
    "img_cols = 376//2\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "bs = 8 # batch size\n",
    "\n",
    "S = cv2.imread(style_image)\n",
    "S = cv2.resize(S, img_shape[:2])\n",
    "\n",
    "# bgr->rgb\n",
    "plt.imshow(S[:,:,::-1]) \n",
    "\n",
    "# tensor to evaluate gram matrix\n",
    "tf_style = tf.convert_to_tensor(S[None], dtype='float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = Input((img_cols, img_rows, channels), name = 'InputImage')\n",
    "\n",
    "\n",
    "'''\n",
    "Now we load a pre-trained VGG16 model, in order to build our perceptual loss.\n",
    "let:\n",
    "S - style loss (MSE between the Gram Matrix of the VGG's layers output of the style image and the generated image)\n",
    "C - content loss (MSE between the VGG's layer\\s output of the original image and the generated). In other words, \n",
    "    we want the VGG model to recognize the same objects in the generated image as in the original.\n",
    "V - variation loss (smoothness of the generated image)\n",
    "\n",
    "our final loss is L = w1 * S + w2 * C + w3 * V\n",
    "'''\n",
    "\n",
    "vgg = VGG16(include_top=False, weights='imagenet', \n",
    "            input_tensor=Lambda(vgg_preprocess_input)(original))\n",
    "for l in vgg.layers: l.trainable = False\n",
    "\n",
    "vgg_content = Model(original, get_output(vgg, 5), name = 'Content-VGG16')\n",
    "vggc1 = vgg_content(original) # content of original image\n",
    "vgg_style = Model(original, [get_output(vgg, o, 1) for o in [1,2,3,4,5]], name = 'Style-VGG16')\n",
    "vggs1 = vgg_style(tf_style) # content of style image - will be used to evaluate the style loss\n",
    "\n",
    "# Now we calculate the gram matrix of every VGG's layer output when we pass the style image to it.\n",
    "S = []\n",
    "for i in range(len(vggs1)):\n",
    "    S.append(gram_matrix(vggs1[i]))\n",
    "\n",
    "def style_loss(x):\n",
    "    style_loss = 0\n",
    "    n=len(x)\n",
    "    for i in range(n):\n",
    "        _, w, h, channels = K.int_shape(x[i])\n",
    "        size = w * h\n",
    "        C = gram_matrix(x[i]) # calculate the gram matrix of every VGG's layer output\n",
    "        loss = K.sum(K.square(S[i] - C), axis = [1,2]) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "        style_loss += loss/n\n",
    "    return K.expand_dims(style_loss, 0)\n",
    "\n",
    "generator = get_generator(img_shape=(img_cols, img_rows, channels)) # load generator model - generating styled images\n",
    "stylish = generator(original)\n",
    "vggc2 = vgg_content(stylish) # content of style\n",
    "vggs2 = vgg_style(stylish) # style of stylished\n",
    "\n",
    "\n",
    "# Create Losses\n",
    "loss_con = Lambda(content_fn, name = 'Content')([vggc1, vggc2])\n",
    "loss_style = Lambda(style_loss, name = 'Style')(vggs2)\n",
    "loss_var = Lambda(total_variation_loss, name = 'Variation')(stylish)\n",
    "\n",
    "model = Model(original, [loss_con, loss_style, loss_var], name = 'StyleTransfer')\n",
    "\n",
    "style_weight = 0.001 # w1\n",
    "content_weight = 1000 # w2\n",
    "total_variation_weight = 0.001 # w3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_gen = ImageLoader(glob(path), img_shape[:2], bs, flip=False)\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.02, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss=['mse','mae','mae'], \n",
    "              loss_weights=[content_weight, style_weight, total_variation_weight])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(data_gen, steps_per_epoch=len(data_gen)//bs, epochs=200, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageLoader(glob(path), img_shape[:2], bs, flip=False)\n",
    "data_gen = iter(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Check models performance '''\n",
    "\n",
    "x,_ = next(data_gen)\n",
    "\n",
    "style = generator.predict(x)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[1][:,:,::-1]/255)\n",
    "plt.subplot(122)\n",
    "plt.imshow(style[1][:,:,::-1]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"model.h5\")\n",
    "print('model {} saved.'.format(style_image[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
